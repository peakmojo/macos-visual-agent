<div align="center">

# ğŸ¯ Visual Agent

### *Your Mac, but it actually understands what's on screen*

**Screen understanding using Apple's Accessibility APIsâ€”soon powered by local LLMs**

[ğŸš€ Demo](#demo) â€¢ [âš¡ Quick Start](#quick-start) â€¢ [ğŸ§  How It Works](#how-it-works) â€¢ [ğŸ” Privacy](#privacy)

![macOS](https://img.shields.io/badge/macOS-13.0+-000000.svg?style=flat&logo=apple)
![Swift](https://img.shields.io/badge/Swift-5.9-FA7343.svg?style=flat&logo=swift)
![ScreenCaptureKit](https://img.shields.io/badge/ScreenCaptureKit-Native-blue.svg?style=flat)
![Accessibility](https://img.shields.io/badge/Accessibility%20API-Native-purple.svg?style=flat)
![License](https://img.shields.io/badge/license-MIT-green.svg?style=flat)

</div>

---

## ğŸ¬ Demo

> *Coming soon: GIF showing the overlay in action, UI tree extraction, and activity timeline*

**What you'll see:**
- Floating minimal overlay that tracks your work in real-time
- Accessibility API extracting UI elements from any app (buttons, text fields, menus)
- Activity timeline showing your workflow patterns
- All running locally at 1 FPSâ€”no cloud, no tracking
- **Soon:** Local LLM understanding full screen context

---

## ğŸ’¡ Why This Exists

Most productivity tools are either:
- ğŸš« **Invasive** (uploading your screen to the cloud)
- ğŸš« **Limited** (only track app names, not context)
- ğŸš« **Closed-source** (you have no idea what they're doing with your data)

**Visual Agent is different:**
- âœ… 100% local processing using Apple's native Accessibility APIs
- âœ… Works with ANY applicationâ€”extracts actual UI structure, not just pixels
- âœ… Fully open sourceâ€”audit every line of code
- âœ… Built by developers, for developers
- âœ… **Coming soon:** Local LLM integration for semantic understanding

### The Problem It Solves

Ever wondered:
- *"How much time did I actually spend focused today?"*
- *"What was I working on 2 hours ago?"*
- *"Which apps are killing my productivity?"*

Traditional time trackers only see app names. **Visual Agent sees structure**â€”the actual UI elements, window layouts, and soon, semantic meaning via local LLMs.

---

## âš¡ Quick Start

```bash
# Clone and build
git clone https://github.com/yourusername/macos-visual-agent.git
cd macos-visual-agent
open VisualAgent.xcodeproj

# Grant permissions when prompted (Screen Recording + Accessibility)
# Launch from Xcode or build for Release
```

**That's it.** The floating overlay appears in your top-right corner.

---

## ğŸ§  How It Works

### The Intelligence Pipeline

```
Screen Capture (1 FPS)
    â†“
Accessibility APIs â†’ Extract UI tree (buttons, inputs, text, menus)
    â†“
Vision Framework â†’ Capture visible text with coordinates
    â†“
Window Manager â†’ Track active apps & window layouts
    â†“
[COMING SOON] Local LLM â†’ Semantic understanding of screen context
    â†“
In-Memory Processing â†’ Real-time insights (persistence coming soon)
```

### Under the Hood

This isn't some Electron wrapper running a Chrome browser. It's **pure native Swift** using Apple's most powerful APIs:

| Framework | What It Does |
|-----------|-------------|
| **Accessibility APIs** | Extracts complete UI treeâ€”every button, text field, menu with exact coordinates |
| **ScreenCaptureKit** | Captures display at 1 FPS (macOS 12.3+) for visual context |
| **Vision Framework** | Detects text regions and extracts content with word-level bounding boxes |
| **SwiftUI** | Native, buttery-smooth 120Hz interface |
| **[Coming]** Local LLM | Ollama/MLX integration for semantic screen understanding |
| **[Coming]** Vector DB | Persistent storage with semantic search capabilities |

**Performance:** Uses ~50MB RAM, <2% CPU on Apple Silicon.

---

## ğŸ¨ What Makes This Special

### 1. **Accessibility-First Architecture**
Unlike screen scraping or pixel-based hacks, Visual Agent uses Apple's **Accessibility APIs**â€”the same system VoiceOver uses:
- Extract complete UI hierarchies from any app
- Get precise element types (button, checkbox, text field, etc.)
- Know exact coordinates and states
- Works even with custom UI frameworks

**Why this matters:** Way more accurate than traditional methods. Works with native apps, Electron apps, web appsâ€”everything.

### 2. **Local LLM Ready**
The architecture is designed for **local AI integration**:
```
Screen Context â†’ UI Tree + Visual Data â†’ Local LLM â†’ Semantic Understanding
```

Imagine:
- "Show me all code-related activity from yesterday"
- "What documentation was I reading when I wrote this function?"
- "Auto-tag my work sessions by project context"

**Privacy-first:** Your screen data never leaves your Mac. Run Ollama or MLX models locally.

### 3. **Ridiculously Extensible**
Want to build:
- A personal search engine of everything you've seen?
- RAG system with your entire work context?
- AI copilot that sees your full development environment?
- Context-aware automation ("when Figma opens, show design system docs")?

**You can.** The architecture is modularâ€”just plug into `ContextStreamManager`.

---

## ğŸ” Privacy

### What This App Does NOT Do

- âŒ **No keystroke logging** (removed in security audit)
- âŒ **No network requests** (check the codeâ€”zero external API calls)
- âŒ **No cloud uploads** (everything stays on your Mac)
- âŒ **No telemetry or tracking** (not even anonymous analytics)
- âŒ **No persistent storage yet** (currently in-memory only)

### What It DOES Collect (100% Locally, In-Memory)

- âœ… UI element metadata â†’ button labels, text fields, window titles
- âœ… Screen captures â†’ processed for visual context, then discarded
- âœ… Text regions â†’ what's visible and where
- âœ… App usage patterns â†’ which apps you're using when

**Current state:** All data is in-memory and lost when you quit the app.

**Coming soon:** Optional local persistence with vector embeddings for semantic search.

### For the Paranoid (We Love You)

```bash
# Verify zero network activity
sudo lsof -i -P | grep VisualAgent  # Should return nothing

# Audit the code yourself
grep -r "URLSession\|fetch\|http" VisualAgent/  # Zero API calls

# Check for any data files
find ~/Library -name "*visualagent*" -o -name "*VisualAgent*"  # Currently none
```

**When LLM support arrives:** All inference runs locally via Ollama or MLX. Your data never touches the internet.

---

## ğŸš€ Use Cases

**For Developers:**
- Track context switches: Xcode â†’ docs â†’ StackOverflow â†’ Slack
- See actual productivity patterns beyond "Chrome was open for 4 hours"
- Build AI dev tools that understand your full environment
- Create personal knowledge base from your screen history

**For Researchers:**
- Study UI/UX patterns in real applications
- Log screen interactions for user studies (with consent)
- Analyze accessibility compliance across apps
- Build datasets of human-computer interaction

**For Hackers & Tinkerers:**
- Personal "time machine" search of everything you've seen
- Auto-journal your workday based on actual screen context
- Context-aware automation and workflows
- Train local AI models on your work patterns

---

## ğŸ› ï¸ Architecture for Contributors

```
VisualAgent/
â”œâ”€â”€ ğŸ“¸ ScreenCaptureManager.swift      â†’ ScreenCaptureKit wrapper (1 FPS)
â”œâ”€â”€ ğŸ¯ AccessibilityAnalyzer.swift     â†’ UI tree extraction via AX APIs
â”œâ”€â”€ ğŸ§  VisionTextExtractor.swift       â†’ Text detection with coordinates
â”œâ”€â”€ ğŸ”„ ContextStreamManager.swift      â†’ Pipeline coordinator
â”œâ”€â”€ ğŸ¨ ContentView.swift               â†’ SwiftUI overlay interface
â””â”€â”€ [Coming] LLMContextProcessor.swift â†’ Local LLM integration
```

**Current Stack:**
- Accessibility APIs for UI structure
- Vision framework for text + coordinates
- Native screen capture at 1 FPS
- In-memory state management

**Coming Soon:**
- Ollama integration for local LLaMA/Mistral models
- MLX support for Apple Silicon optimized inference
- Vector database for persistent semantic search
- RAG pipeline for screen memory

**Want to contribute?**
- Help integrate Ollama/MLX for local LLM support
- Design the persistence layer (vector DB, embeddings)
- Build export plugins (Obsidian, Notion, CSV)
- Create visualization tools for screen timelines
- Multi-monitor support

No webpack. No npm. No bullshit. Just Swift + Xcode.

---

## ğŸ¯ Roadmap

**Phase 1: Native Foundation** (âœ… Done)
- [x] Screen capture at 1 FPS via ScreenCaptureKit
- [x] Accessibility API integration for UI tree extraction
- [x] Vision framework for text detection
- [x] Floating overlay interface
- [x] In-memory activity tracking

**Phase 2: Persistence & Intelligence** (ğŸš§ Next)
- [ ] Local persistence layer (vector DB or similar)
- [ ] Smart activity categorization (coding vs. browsing vs. meetings)
- [ ] Export formats (JSON, CSV, Markdown)
- [ ] Data retention policies (auto-cleanup after N days)
- [ ] Encryption for stored data

**Phase 3: Local AI Superpowers** (ğŸ’¡ Planned)
- [ ] **Ollama integration** â†’ Run LLaMA 3.3, Mistral locally
- [ ] **MLX support** â†’ Apple Silicon optimized inference
- [ ] **Vector embeddings** â†’ Semantic understanding of screen context
- [ ] **RAG pipeline** â†’ "Search everything I've seen this week about React hooks"
- [ ] **Semantic tagging** â†’ Auto-categorize sessions by project/topic
- [ ] **Context-aware insights** â†’ "You're most productive in morning sessions when..."

**Phase 4: Ecosystem** (ğŸ”® Future)
- [ ] Plugin system for custom analyzers
- [ ] Multi-display support
- [ ] Team features (opt-in collaborative insights)
- [ ] API for third-party integrations

---

## ğŸ¤ Contributing

**We want your ideasâ€”especially around local LLM integration.**

Working on Ollama/MLX/llama.cpp? Want to help make this the best local-first productivity tool?

1. **Open an issue** firstâ€”let's discuss your idea
2. **Fork & build** your changes
3. **Submit a PR** with clear description

**Priority areas:**
- Local LLM integration (Ollama, MLX)
- Persistence layer design (vector DB, embeddings)
- RAG implementation for screen memory
- Privacy-preserving analytics
- Export & visualization tools

**No bureaucracy. No corporate approval. Just ship.**

---

## ğŸ“œ License

MIT Licenseâ€”build products, fork it, sell it, integrate it, we don't care.

Just keep it local. Keep it open. Keep it honest.

---

<div align="center">

### â­ If local-first AI tools matter to you, star this.

### ğŸ´ If you want to build something with screen context, fork it.

### ğŸš€ If you're working on local LLMs, let's collaborate.

---

**Built with â¤ï¸ by developers who believe your screen data belongs on YOUR machine.**

**Not in some cloud. Not feeding someone's AI training pipeline. Just yours.**

[Report Bug](https://github.com/yourusername/macos-visual-agent/issues) â€¢ [Request Feature](https://github.com/yourusername/macos-visual-agent/issues) â€¢ [Discussions](https://github.com/yourusername/macos-visual-agent/discussions)

</div>
